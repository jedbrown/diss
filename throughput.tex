\begin{quote}
  \emph{The easiest way to make software scalable is to make it sequentially inefficient.}~\citep{gropp1999exploiting}
\end{quote}

\subsection{Sparse matrix kernels}\label{ssec:sparsekernels}

This section briefly describes some optimizations that improved PETSc's matrix kernels by about 30\% on Intel and AMD architectures, bringing them quite close to the memory bandwidth limits.
This was implemented subsequent to the new data structures for factored matrices described in \citet{smith2010sparse} and thus represents an additional improvement.

The following hardware descriptions generally hold across Intel Core, Core 2, and Core i7 as well as AMD K8 and K10 microarchitectures.
Most variation is due to different prefetch and TLB semantics as well as different latencies for vector instructions.
Details can be found in the optimization reference manuals~\citep{intel2011optimization,amd2009optimization} as well as Agner Fog's excellent resources~\citet{fog2011michoarchitecture,fog2011instruction}.
See \citet{drepper2007memory} for a more accessible introduction to the memory hierarchy.

Most of our optimizations involve maximal reuse of level 1 data (L1D) cache, higher level caches, and streaming bandwidth.
Each floating point unit can issue one packed add and one packed multiply per clock cycle, with latencies of 3 to 5 cycles.
SSE instructions may have both operands in registers or have one operand in L1D with no throughput penalty, although a concurrently issued add and multiply cannot both have a memory operand.
AMD has a 2-cycle latency penalty for memory operands, Intel has no latency penalty.
This means that L1D is almost as good as a register, compared to L2 with roughly 10-cycle latency and DRAM with approximately 250-cycle latency.

Assuming a 2.5 GHz clock, one packed load and store per cycle means a bandwidth to L1 of 24 GB/s each way (48 GB/s each way for newer architectures supporting 32-byte AVX registers).
We can compare this to per-core memory bandwidth ranging from 1.75 GB/s (6-core with 2-channel DDR2-667, found on Cray XT-5) to 9.6 GB/s (4-core with 3-channel DDR3-1600, found on Intel's newest ``Sandy Bridge'' processors).
With four (SSE2) or eight (AVX) double precision floating point operations possible per cycle, it becomes clear that memory will be the bottleneck unless many floating point operations can be performed per value loaded.
The ratio of flops per byte for a computational kernel is known as arithmetic intensity.
The systems above balance computation with memory when the arithmetic intensity is between 2 and 6 flops/byte.

The hardware prefetch unit can recognize 16 forward-moving streams and 4 backward-moving streams, but only one stream per 4 kB page and not across page boundaries.
Additionally, the hardware prefetcher does not resolve minor TLB misses which is a significant issue because the TLB can only address 1 or 2 MB and TLB misses produce roughly 60 cycles of latency, plus DRAM latency if the resulting address is not in high-level cache.
Software prefetch can hide these latencies and resolve TLB misses in advance, providing roughly 20\% improvement in our STREAM benchmarks~\citep{mccalpin2007stream}.
In addition to improving overall bandwidth, software prefetch instructions can set a ``non-temporal access'' (NTA) policy such that lines evicted from L1D will not subsequently reside in higher level caches.

We consider four matrix formats with increasing amounts of structure: {\AIJ}, {\AIJInode}, {\BAIJ}, and {\SBAIJ}.
{\AIJ} is a standard compressed row format that stores a column index for each nonzero entry.
A matrix-vector product $y \gets A x$ is characterized by the kernel
\begin{minted}{c}
  for (i=0; i<m; i++) {
      y[i] = 0.0;
      for (j=ai[i]; j<ai[i+1]; j++)
          y[i] += aa[j] * x[aj[j]];
  }
\end{minted}
Assuming perfect cache reuse with $m$ rows and an average of $n$ nonzeros per row, {\MatMult} has an arithmetic intensity of
\begin{align*}
  \I_{\text{AIJ}} = \frac{2n}{(n+1)\texttt{sizeof(Scalar)} + (n+1)\texttt{sizeof(Int)}} \xrightarrow{n\to\infty} 0.167 \text{flops/byte}
\end{align*}
where the long-row limit is calculated for \cverb|double| precision and 32 bit integers.
In practice, the vector will not reuse cache perfectly so some entries of $x$ will need to be loaded into cache more than once.
Choosing a low-bandwidth ordering such as Reverse Cuthill-McKee~\citep{cuthill1969reducing} (RCM) improves cache reuse which reduces end-to-end run time substantially (speeding up matrix-free operations such as residual evaluation in addition to sparse matrix kernels), e.g. by more than a factor of 2~\citep{gropp2000pmt}.
Our optimization for {\AIJ} consisted of inserting software prefetch instructions to initiate loads of every cache line holding entries in the row after the current one.
The L1D cache line length is detected during configuration so only one prefetch instruction per line is issued (it is 64 bytes on the x86-64 microarchitectures considered here).
Sparse matrix-vector products for PDEs can be thought of as a stencil operation combined with a high-volume stream of matrix entries and column indices (the weights and shape of the stencil).
Without the NTA policy, the high-volume stream flushes the vector $x$ out of caches prematurely.

The {\AIJInode} format simply augments the {\AIJ} format by marking clusters of consecutive rows ``Inodes'' that have the same nonzero pattern, then matrix kernels unroll over the Inodes.
Use of the {\AIJInode} format is automatic if the matrix has consecutive rows with the same nonzero pattern.
The column indices are \emph{not} copied out of the standard {\AIJ} storage so matrix kernels skip over $b-1$ rows of identical column indices in each Inode of size $b$.
Skipping over these rows of column indices is especially inefficient with hardware prefetch because entries are eagerly brought into cache and then skipped, followed by a full DRAM stall as the kernel jumps past the prefetched indices in \cverb|aj| and starts $b-1$ new streams from \cverb|aa| that are not predicted by the prefetch unit.
Software prefetch for {\AIJInode} avoids bringing in the redundant column entries and prevents the stalls due to jumping over column indices and starting new \cverb|aa| streams.
The prefetch logic assumes that the next Inode and row lengths will be approximately the same size as the current one since this is the typical case for PDE-like problems.
If no redundant column entries are brought in, the arithmetic intensity is
\begin{align*}
  \I_{{\AIJInode}} = \frac{2nb}{(n+1)b\texttt{sizeof(Scalar)} + (n+2)\texttt{sizeof(Int)}} \xrightarrow[b=3]{n\to\infty} 0.214 \text{flops/byte}
\end{align*}
where the example Inode size of 3 is representative of 3D elasticity or the viscous part of a Stokes problem.
We expect this performance model to be somewhat less sharp than for {\AIJ} because it is not possible to load less than a cache line, therefore redundant column indices are unavoidable unless all rows naturally align with cache line boundaries.

The {\BAIJ} format optimizes for constant block size by storing only one column index per $b\times b$ block.
With this format, the problems of skipping ahead in \cverb|aj| and \cverb|aa| go away, but issues of peak bandwidth and enabling the vector to reuse cache remain.
The arithmetic intensity
\begin{align*}
  \I_{{\BAIJ}} = \frac{2nb}{(n+1)b\texttt{sizeof(Scalar)} + (n/b+1)\texttt{sizeof(Int)}} \to \xrightarrow[b=3]{n\to\infty} 0.237 \text{flops/byte}
\end{align*}
is very close to the dense limit of $0.25$.
In practice, {\BAIJ} achieves a higher fraction of the bandwidth peak due its more regular memory access and unrolled kernels.

The symmetric block format {\SBAIJ} which stores only the upper-triangular part in compressed row format has a more involved memory access pattern.
Each matrix entry is only loaded once where it is used for both upper and lower triangular contributions.
The contribution from the lower-triangular part has multiple destinations effectively doubling the number of stencil-like streams that must be sustained, with all the new streams read-write as opposed to the read-only streams of nonsymmetric row formats.
In return, the storage and arithmetic intensity is almost double that of nonsymmetric formats
\begin{align*}
  \I_{\SBAIJ} = \frac{4nb - 2b^2}{(n+2)b\texttt{sizeof(Scalar)} + (n/b+1)\texttt{sizeof(Int)}} \to \xrightarrow[b=3]{n\to\infty} 0.47 \text{flops/byte} .
\end{align*}
% let scalar = 8; int = 4; intenBAIJ n b = 2*n*b / ((n+1)*b*scalar + (n/b+1)*int); intenAIJInode n b = 2*n*b/((n+1)*b*scalar + (n+2)*int)

The data structure change described in \citet{smith2010sparse} caused both forward and back solves to traverse matrix entries forward in memory when using (incomplete) LU.
Even though the matrix entries are fetched moving forward, the stencil for ``back solve'' still moves backward through memory so there are much fewer hardware prefetch streams (4 instead of 16 forward-moving on Intel/AMD, there is no hardware prefetch for backward-moving streams on Blue Gene/P).

For (incomplete) Cholesky, the factors are only stored once so it is not possible for both forward and back solves to traverse matrix entries by moving forward.
Because the primary backward-moving streams are prefetched by software, this causes relatively little performance degradation compared to ILU.

% Core 2 P8700: DDR2-800 (6400 MB/s), single thread Triad peak 5400 MB/s
% gcc-4.7 -O3 -std=c99 -march=native BasicVersion-onnode.c -lm -lnuma -DSSE2 -DPREFETCH_NTA
% I observed as high as 5500 MB/s when using -DFAULT_TOGETHER, but this
% is not practical
% Performance is dependent on how the memory was used the last time
% through, therefore optimization of each STREAM kernel is not entirely
% independent of the others.

As an estimate of the peak usable bandwidth, we use the STREAM Triad benchmark with a moderately tuned implementation (software prefetch, SSE2 arithmetic, non-temporal stores, unrolled over cache lines).
We consider two test systems, a Core 2 Duo (P8700) clocked at 2.53 GHz with a single DDR2-800 memory channel (theoretical bandwidth of 6.4 GB/s) and a two-socket Opteron 2356 (quad core) clocked at 2.3 GHz with two channels of DDR2-677 memory per socket (theoretical peak of 5.3 GB/s per channel).
The achieved bandwidth for Triad on the Intel system was 5.4 GB/s for a single thread.
On Opteron, the single-threaded Triad achieved 5.8 GB/s, increasing to \todo{XXX} GB/s for 4 processes (2 per socket) and \todo{XXX} GB/s for 8 processes.

Table~\ref{tab:throughput:baij} shows the performance of the three primary sparse matrix kernels with each storage format on both test machines.
With prefetch enabled, the sparse matrix kernels obtain a remarkably high fraction of theoretical peak bandwidth, especially on Intel processors.
Indeed, the scalar format performance is competitive with the best {\MatMult} results of \citet{williams2007osm} and block formats are consistently faster by a factor of 20 to 30 percent.
Block formats provide greater benefit for {\MatSolve} than for {\MatMult}, with {\BAIJ} {\MatSolve} frequently outperforming the corresponding {\MatMult}.
We are not aware of previous reports of this effect and speculate that it is due to {\MatSolve} having fewer read streams and using only one vector at a time ({\MatMult} reads from one vector and writes to another, {\MatSolve} is in-place).

\begin{table}
  \centering
  \begin{tabular}{l c c c c}
    \toprule
                                                   & \multicolumn{4}{c}{Core 2, 1 process}            \\
                                                   & {\AIJ}       & {\AIJInode} & {\BAIJ}      & {\SBAIJ}     \\ \cmidrule{2-5}
    \MatMult                                       & 916(103\%) & 855(81\%) & 1013(86\%) & 1429(62\%) \\
    \MatSolve/ILU                                  & 799        & 742       & 1078       & ---        \\
    \MatSolve/ICC                                  & 741        & 737       & 1011       & 1007       \\ \midrule
                                                   & \multicolumn{4}{c}{Opteron, 1 process}           \\
                                                   & {\AIJ}       & {\AIJInode} & {\BAIJ}      & {\SBAIJ}     \\ \cmidrule{2-5}
    \MatMult                                       & 506(53\%)  & 592(52\%) & 735(58\%)  & 744(30\%)  \\
    \MatSolve/ILU                                  & 460        & 572       & 845        & ---        \\
    \MatSolve/ICC                                  & 426        & 427       & 815        & 814        \\ \midrule
                                                   & \multicolumn{4}{c}{Opteron, 4 processes}         \\
                                                   & {\AIJ}       & {\AIJInode} & {\BAIJ}      & {\SBAIJ}     \\ \cmidrule{2-5}
%% FIXME: STREAM performance using MPI
    \MatMult                                       & 1673       & 1978      & 2450       & 3089       \\
    \MatSolve/ILU                                  & 1621       & 1896      & 2766       & ---        \\
    \MatSolve/ICC                                  & 1519       & 1522      & 2700       & 2710       \\ \midrule
                                                   & \multicolumn{4}{c}{Opteron, 8 processes}         \\
                                                   & {\AIJ}       & {\AIJInode} & {\BAIJ}      & {\SBAIJ}     \\ \cmidrule{2-5}
%% FIXME: STREAM performance using MPI
    \MatMult                                       & 2374       & 2408      & 2897       & 4331       \\
    \MatSolve/ILU                                  & 2321       & 1954      & 2913       & ---        \\
    \MatSolve/ICC                                  & 2214       & 1854      & 2892       & 2877       \\
    \bottomrule
  \end{tabular}
  \caption{Throughput (Mflop/s) and percentage of STREAM Triad bandwidth, assuming optimal vector reuse, for three matrix kernels and different matrix formats.
    The test machines are a Core 2 Duo (P8700) and an Opteron 2356 (two sockets).
    The test problem is a $Q_1$ finite element discretization with block size of 2 on a 3D mesh (essentially a 27-point stencil) in which each process has $64\times 64\times 63$ nodes.}\label{tab:throughput:baij}
\end{table}

\subsection{Small dense tensor product kernels}\label{ssec:tensor}
A fundamental operation for computing with high order finite element and spectral element methods is application of a 3D tensor product kernel operations $y \gets (A\otimes B\otimes C) x$, or, in index notation,
\begin{equation}\label{eq:tensor:kernel}
  y_{ijk} \gets A_{i\alpha} B_{b\beta} C_{c\gamma} x_{\alpha\beta\gamma} .
\end{equation}
To evaluate a state vector at quadrature points, the Greek indices run over the basis functions in each Cartesian direction and the Latin indices run over the number of quadrature points in that direction.
When using a Gauss quadrature that integrates the mass matrix exactly (a typical choice in finite element computations), the range is the same.
Additionally, when the approximation order is isotropic (the most common case), all the ranges are the same.
The tensor product operation can be written in various ways as dense matrix multiplication (albeit highly non-square) so we expect it to achieve very close to the floating point peak for sufficiently large sizes (e.g. by calling a tuned BLAS3).
Increasing the number of fields also increases the problem size which improves the efficiency of BLAS3 calls.
The attainable performance for smaller sizes is much less clear, hence we examine the scalar $\Qk 3$ case.
This is a very practical approximation order: it is high enough to reap some benefits from the regular structure, but not so high as to make resolving geometry overly difficult or to have highly non-uniform resolution (due to clustering of interpolation nodes near the edges of elements as the approximation order increases).

In the scalar $\Qk 3$ case, each of $A,B,C$ is $4\times 4$ and the entire operation produces 64 entries in $y$ from 64 entries in $x$.
We have considered a variety of unroll-and-jam optimizations as well as different register blocking strategies.
The SSE3 horizontal add instruction \asm{HADDPD} has increased latency and reduced throughput compared to the standard vector addition and multiplication instructions \asm{ADDPD} and \asm{MULPD}, therefore we should try to avoid it whenever possible.
Assuming lexicographic ordering, applying the first two parts $A$ and $B$ of the tensor product kernel perform the same operation for all values of $\gamma$ and thus provide balanced adds and multiplies (which can be issued concurrently as long as the kernel has been unrolled enough to cover the latency) without needing any horizontal operations.
This is not possible when applying $C$ because packed loads (and vector arithmetic with memory operands) retrieve consecutive entries.
The input could be transposed to avoid this constraint, but the cost for small sizes overwhelms the benefit, therefore we are forced to use one \asm{HADDPD} instruction per packed result $y_{i,j,k:k+1}$ and the reduction also creates some unavoidable (without more registers) pipeline stalls.
Application of $A\otimes B\otimes \bm 1$ achieve nearly 80\% of floating point peak on the Core 2 Duo while $C$ is reduced to near 60\%.
The full tensor product contraction $A\otimes B\otimes C$ achieves more than 70\% of floating point peak on Intel and about 60\% on AMD.
This performance is comparable to or better than the best autotuned results of \citet{shin2010speeding}, though they achieve slightly higher performance for larger sizes (which have less overhead).
Note that large size tensor contractions are important for quantum chemistry and have thus received more optimization attention \citep[\eg][]{kaushik2008improving,hirata2003tensor}.
Contrast the tensor product performance with sparse matrix kernels that attain less than 10\% of floating point peak due to low arithmetic intensity.

An old \citep{mccalpin2007stream} but continuing trend in computer architecture is for peak floating point performance to continue to improve via the use of longer registers (Intel's AVX and the Blue Gene/Q architecture have packed 32-byte floating point registers), more cores, and GPU-style vectorization while memory bandwidth increases much more slowly~\citep{keyes2011exaflop}.
An additional feature of upcoming high-performance computing architectures is the prevalence of in-order execution; high power requirements no longer justify the luxury of sophisticated out-of-order execution units common on today's commodity hardware~\citep{seiler2008larrabee,pham2006overview}.
Achieving high performance on such systems requires decomposing the computation into kernels with high arithmetic intensity (so as not to overload the memory subsystem) and sufficient local structure to utilize vector registers and effectively schedule instructions at compile time.
We optimized streaming stencil kernels for Blue Gene/P in \citet{malas2011streaming}, including implementation of a static scheduler that we used to rapidly perform loop optimizations with a small assembly-language building block that exploited the available SIMD floating point unit.
With this approach, we were able to effectively use all the floating point and general purpose registers to hide instruction latency, delivering 93\% of theoretical FPU peak for the 27-point stencil with problem sizes that fit in level 1 cache and 91\% of bandwidth for larger problem sizes (72\% of FPU peak).
These results are 80\% better than the best previously published for Blue Gene/P.

Since the tensor product kernel has sufficient local structure to utilize vector registers, we can estimate its performance on future architectures by computing its arithmetic intensity.
For simplicity, suppose that $n = p+1$ is both the number of interpolation nodes and quadrature points on an element in each Cartesian direction, and that $b$ is the number of degrees of freedom per node.
Then direction of a contraction performs $bn^4$ multiplies and $b(n-1)n^3$ additions on the reference element.
We approximate the total across all three dimensions $3bn^4$ fused multiply-add (FMA) operations because FMA units are increasingly popular.
When evaluating both interpolation and gradient (most general form), it is possible to share partial results so that both can be evaluated in $9bn^4$ FMAs.
Translating the gradient from reference to physical element costs another $9bn^3$ FMAs.
The Galerkin procedure evaluates these contractions twice per residual or matrix-free Jacobian (once to evaluate the trial function, once more in transpose for the test functions), leading to a total of $18(bn^4 + bn^3) + Qn^3$ FMAs, where $Q$ is the number of operations per quadrature node.
Typically we do not store the coordinate transformation, so it must also be computed at a cost of $9\cdot 3n^4 + 31n^3$ (assume block size 3 and held in an equal order basis, evaluate only on the reference element, then invert the local Jacobian at every quadrature node at a cost of 30 FMA or multiplies and one division).
Thus, for a problem with $b$ degrees of freedom per node and no sharing between elements, we have total operation count of $9(3 + 2b)n^4 + (31+18b + Q)n^3$ with $(b+3)n^3$ floating point values coming in and $bn^3$ out.

To understand these tradeoffs, we consider three block sizes at different approximation orders and plot the memory bandwidth and number of flops (calculated as twice the number of FMAs) required to compute each entry in the output vector.
The performance model for assembled sparse matrices assumes {\BAIJ} storage and perfect vector reuse within the cache system.
The model for unassembled storage applied by tensor product assumes a general tensor-product quadrature of equal order, that coordinates are stored in a function space of the same order as the independent variables and are needed to compute the coordinate transformation but do not contribute to a result, that the physics can be represented using stored data equivalent in size to a stored gradient plus stored function values, and that the physics can be applied using an operation similar in structure to Newton-linearization of Navier-Stokes with power-law rheology plus a reaction term coupling all components.
This is intended to be nearly a worst-case setting for the unassembled tensor product formulation; it is cheaper for simpler physics, simpler coefficients, collocated quadratures, etc.

\begin{figure}
  \centering
  \includegraphics{TensorVsAssembly}
  % python2 ./spmvmodel.py --plot --format pdf -o TensorVsAssembly.pdf
  \caption{Memory and floating point requirements for matrix-free tensor-product application of an operator versus representation as an assembled matrix stored in \BAIJ[b] format.
    The same operation $y \gets A x$ is applied in both cases, the storage is just different.
    A ``result'' is a single scalar entry in $y$, regardless of the block size $b$.}\label{fig:tensorasm}
\end{figure}

The memory and floating point costs are shown in \figref{fig:tensorasm}.
The unassembled representation is uniformly better in terms of memory use for all orders $p \ge 2$, but has significant floating point overhead for the smallest sizes, especially with small block size.
Note that both bandwidth and computation requirements improve for the tensor product formulation when the block size increases (because the overhead of the coordinate transformation is reduced), while they degrade for the assembled representation.
The arithmetic intensity for unassembled representations is typically around 8 flops/byte with weak dependence on polynomial order, block size, and even details of the physics (unless a great deal of recomputation is required for linearization, or a nonsymmetric high-rank tensor needs to be stored).
This should be compared to the assembled sparse matrix representation which cannot surpass 1 flop/4 bytes even if column indices were not stored.
As noted in \secref{ssec:sparsekernels}, modern hardware balances computation with memory when the arithmetic intensity is between about 2 and 6 flops/byte and expected to increase.
The 30-fold increase in arithmetic intensity while simultaneously reducing memory traffic below even the lowest order representation bodes well for the relevance of unassembled Jacobian representations on future architectures.
